---
title: "Model Comparison in Predicting Cholesterol Levels"
subtitle: "P8106 Midterm Project"
author: "Adeline Shin (as5951)"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1)
library(tidyverse)
library(caret)
library(ModelMetrics)
library(glmnet)
library(bootstrap)
```

# Introduction
In the US, high cholesterol is a common health problem, affecting more than 12% of adults over the age of 20, according to the CDC. This data on cholesterol was collected as a part of the NHANES database, which consists of answers to a national survey conducted on nutrition and health behavior among Americans.

Our group used the NHANES data to predict cholesterol based on demographic, dietary, laboratory, and questionnaire data from the surveys conducted during 2015-2016. We picked a combination of 63 potential predictors from these categories in order to cover potential social, behavioral, and genetic determinants of the outcome variable, LDL cholesterol levels. We were interested in determining which model type was the most effective in predicting cholesterol levels given values for all other predictors.

# Methods
The 63 variables as potential predictors were chosen by looking at the entirety of the NHANES dataset. Based on our research on causes of high cholesterol levels, we decided to pick variables across all sections of the NHANES dataset. The chosen variables are listed in Appendix A1, along with their variables names and categories.

Using these 63 variables, we first separated the data into training and test data using an 80/20 split. The training data was used to generate the models, and the test data was used to compare to the predicted values from the models. Using the RMSE calculated between the test data and the predicted data, we were able to compare which method had the lowest RMSE value, and therefore the best predictive abilities. 

In this project, the caret package in R was used to train all the models, and thus, the models compared were a linear model, a ridge regression model, a lasso model, and an elastic net model with an alpha value of 0.75. Our group members decided to use different cross-validation methods in order to see whether that would make a difference in terms of the final chosen model. In the models below, the 632 bootstrap cross-validation method was used, while other team members chose to use Monte Carlo cross-validation or leave one out cross-validation.

# Results

```{r include = FALSE}
# Loading the Data
training_data = read_csv("./training_data_final.csv") %>% 
  mutate(
    dbd100 = as.factor(dbd100),
    drqsprep = as.factor(drqsprep),
    drqsdiet = as.factor(drqsdiet),
    dr1_300 = as.factor(dr1_300),
    drd340 = as.factor(drd340),
    drd360 = as.factor(drd360),
    bpq020 = as.factor(bpq020),
    cdq001 = as.factor(cdq001),
    hsd010 = as.factor(hsd010),
    diq010 = as.factor(diq010),
    dbq700 = as.factor(dbq700),
    fsd032a = as.factor(fsd032a),
    fsd032c = as.factor(fsd032c),
    fsdhh = as.factor(fsdhh),
    hiq270 = as.factor(hiq270),
    paq605 = as.factor(paq605),
    smq020 = as.factor(smq020),
    riagendr = as.factor(riagendr),
    ridreth1 = as.factor(ridreth1),
    ridreth3 = as.factor(ridreth3),
    dmdborn4 = as.factor(dmdborn4)
  )

test_data = read_csv("./test_data_final.csv") %>% 
  mutate(
    dbd100 = as.factor(dbd100),
    drqsprep = as.factor(drqsprep),
    drqsdiet = as.factor(drqsdiet),
    dr1_300 = as.factor(dr1_300),
    drd340 = as.factor(drd340),
    drd360 = as.factor(drd360),
    bpq020 = as.factor(bpq020),
    cdq001 = as.factor(cdq001),
    hsd010 = as.factor(hsd010),
    diq010 = as.factor(diq010),
    dbq700 = as.factor(dbq700),
    fsd032a = as.factor(fsd032a),
    fsd032c = as.factor(fsd032c),
    fsdhh = as.factor(fsdhh),
    hiq270 = as.factor(hiq270),
    paq605 = as.factor(paq605),
    smq020 = as.factor(smq020),
    riagendr = as.factor(riagendr),
    ridreth1 = as.factor(ridreth1),
    ridreth3 = as.factor(ridreth3),
    dmdborn4 = as.factor(dmdborn4)
  )

# Creating x and y Variables for Test and Training Data
y = training_data$lbdldl
x = model.matrix(lbdldl ~ ., training_data)[, -(1:2)]
y_test = test_data$lbdldl
x_test = model.matrix(lbdldl ~ ., test_data)[, -(1:2)]
```

## Linear Model
```{r echo = FALSE}
set.seed(13)
ctrl1 = trainControl(method = "boot632")

linear_fit = train(lbdldl ~ .,
                   data = training_data,
                   method = "lm",
                   trControl = ctrl1)

summary(linear_fit)
```

As apparent from the summary of the linear model, many of the predictors are not significant at a 95% significance level. This likely means that the model does not fit the data well, and therefore, the model would not predict the value of cholesterol well.

### Prediction using Linear Model
```{r echo = FALSE}
lm_prediction = predict(linear_fit, newdata = test_data)
lm_mse = mse(y_test, lm_prediction)
```

The linear model has an MSE of `r lm_mse`, which is high, but expected, as the model itself did not have many significant variables.

## Ridge Regression Model
```{r echo = FALSE}
set.seed(13)
ctrl1 = trainControl(method = "boot632")

ridge_fit = train(lbdldl ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 0, 
                                         lambda = exp(seq(-0, 5, length=100))),
                  preProc = c("center", "scale"),
                  trControl = ctrl1)

plot(ridge_fit, main = "RMSE vs. Lambda for Ridge Regression")

ridge_coef = coef(ridge_fit$finalModel, ridge_fit$bestTune$lambda)
ridge_coef_sum = sum(as.vector(ridge_coef) != 0)
```

As shown on the graph above, the value of lambda that gives the lowest RMSE value is `r ridge_fit$bestTune$lambda`. The ridge model at this value of lambda gives `r ridge_coef_sum - 1` variables in the final model, which can then be used to predict cholesterol levels.

### Prediction using Ridge Model
```{r echo = FALSE}
ridge_prediction = predict(ridge_fit, newdata = test_data)
ridge_mse = mse(y_test, ridge_prediction)
```

The ridge model gives an MSE of `r ridge_mse`, which is lower than that of the linear model.

# Lasso Model
```{r echo = FALSE}
set.seed(13)
ctrl1 = trainControl(method = "boot632")

lasso_fit = train(lbdldl ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 1, 
                                         lambda = exp(seq(-1, 2, length=100))),
                  preProc = c("center", "scale"),
                  trControl = ctrl1)

plot(lasso_fit, main = "RMSE vs. Lambda for Lasso Regression")

lasso_coef = coef(lasso_fit$finalModel, lasso_fit$bestTune$lambda)
lasso_coef_sum = sum(as.vector(lasso_coef) != 0)
```

As shown on the graph above, the value of lambda that gives the lowest RMSE value is `r lasso_fit$bestTune$lambda`. The lasso model at this value of lambda gives `r lasso_coef_sum - 1` variables in the final model, which can then be used to predict cholesterol levels.

### Prediction using Lasso Model
```{r echo = FALSE}
lasso_prediction = predict(lasso_fit, newdata = test_data)
lasso_mse = mse(y_test, lasso_prediction)
```

The lasso model gives an MSE of `r lasso_mse`. 

## Elastic Net Model
```{r echo = FALSE}
set.seed(13)
ctrl1 = trainControl(method = "boot632")

elastic_net_fit = train(lbdldl ~ .,
                        data = training_data,
                        method = "glmnet",
                        tuneGrid = expand.grid(alpha = 0.75, 
                                          lambda = exp(seq(-0, 2, length=100))),
                        preProc = c("center", "scale"),
                        trControl = ctrl1)

plot(elastic_net_fit, main = "RMSE vs. Lambda for Elastic Net")

elastic_net_coef = coef(elastic_net_fit$finalModel, elastic_net_fit$bestTune$lambda)
elastic_coef_sum = sum(as.vector(elastic_net_coef) != 0)
```

As shown in the graph above, the elastic net model with an alpha of 0.75 has the lowest RMSE at a lambda value of `r elastic_net_fit$bestTune$lambda`. This combination of alpha and beta gives a model that contains `r elastic_coef_sum - 1` predictors in the final model. This model is then used to predict the cholesterol level and compared to the test data.

### Prediction using Elastic Net 
```{r echo = FALSE}
elastic_prediction = predict(elastic_net_fit, newdata = test_data)
elastic_mse = mse(y_test, elastic_prediction)
```

Prediction of cholesterol levels using the elastic net model gives an MSE of `r elastic_mse`. 

## Model Comparison
```{r echo = FALSE}
resamp = resamples(list(lm = linear_fit,
                        ridge = ridge_fit,
                        lasso = lasso_fit,
                        elastic = elastic_net_fit))

resamp_summary = summary(resamp)

bwplot(resamp, metric = "RMSE", main = "RMSE comparison of 4 Models")
```

As shown in the box plot above, the model created using the elastic net  gives the lowest RMSE among the four models created for the purpose of predicting cholesterol levels. Therefore, the elastic model should be chosen when using 632 bootstrap as the cross validation method to predict cholesterol levels.
